{"title":"Llama 2 本地部署指南","date":"2023-11-24T00:00:00.000Z","date_formatted":{"ll":"Nov 24, 2023","L":"11/24/2023","MM-DD":"11-24"},"thumbnail":"https://blog-1251959181.cos.accelerate.myqcloud.com/cover/llama.png","link":"2023/11/23/oc_run_llama2_local","comments":true,"tags":["Original Content"],"categories":["Coding"],"updated":"2024-07-01T17:57:02.538Z","content":"<p>近期对 <code>llama2</code> 进行了本地部署测试，鉴于中文互联网相关实战资料较少，官方文档更新不及时，将个人踩坑经验整理如下，仅供参考。</p>\n<ul>\n<li>环境: MacBook Pro 14英寸 2023年</li>\n<li>芯片: Apple M2 Pro</li>\n<li>内存: 16 GB</li>\n<li>系统：macOS Sonoma 14.0</li>\n</ul>\n<hr>\n<h3 id=\"下载模型文件\">下载模型文件<a title=\"#下载模型文件\" href=\"#下载模型文件\"></a></h3>\n<p>访问<a href=\"https://ai.meta.com/\" target=\"_blank\">meta</a> 和 <a href=\"https://ai.meta.com/llama/\" target=\"_blank\">llama</a> 网站可以查看相关信息</p>\n<p>点击下载模型按钮将跳转到<a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\" target=\"_blank\">请求下载页面</a></p>\n<p>填写表单，滑动到页面底部勾选同意并接受条款，就可以获取下载链接，会以邮件的形式发送到填写的邮箱当中。</p>\n<p>从 GitHub 克隆 <a href=\"https://github.com/facebookresearch/llama\" target=\"_blank\">facebookresearch/llama</a> 仓库代码到本地，这只是一个安装脚本，下载很快</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/facebookresearch/llama.git</span><br><span class=\"line\"><span class=\"built_in\">cd</span> llama</span><br><span class=\"line\">./download.sh</span><br></pre></td></tr></table></figure>\n<p>将邮件中的链接复制下来，并填写要下载的模型，就可以开始下载了</p>\n<blockquote>\n<p>模型文件很大，不需要科学上网；建议找网络环境好的地方，速度还是很快的</p>\n</blockquote>\n<h3 id=\"安装-llama.cpp\">安装 llama.cpp<a title=\"#安装-llama.cpp\" href=\"#安装-llama.cpp\"></a></h3>\n<p>要在 MacOS 、 Windows 等系统上运行 Llama ，需要借助开源项目 <a href=\"https://github.com/ggerganov/llama.cpp\" target=\"_blank\">llama.cpp</a></p>\n<p>首先在合适位置下载项目并构建</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/ggerganov/llama.cpp.git</span><br><span class=\"line\"><span class=\"built_in\">cd</span> llama.cpp</span><br><span class=\"line\">make</span><br></pre></td></tr></table></figure>\n<p>构建完成以后在<code>llama.cpp</code>目录会多一系列文件，比较重要的是<code>main</code>和<code>quantize</code></p>\n<h3 id=\"准备数据\">准备数据<a title=\"#准备数据\" href=\"#准备数据\"></a></h3>\n<p>将下载的 llama 模型的原始权重放置在 <code>llama.cpp</code> 项目的 <code>./models</code> 目录下。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">user@mbp llama.cpp: <span class=\"built_in\">ls</span> ./models</span><br><span class=\"line\">7b 7b-chat tokenizer.model tokenizer_checklist.chk</span><br></pre></td></tr></table></figure>\n<p>确保 <code>models</code> 目录包含上述文件</p>\n<h3 id=\"转换模型格式\">转换模型格式<a title=\"#转换模型格式\" href=\"#转换模型格式\"></a></h3>\n<p>将 7B 模型转换为 ggml FP16 格式，然后使用 4-bits 量化（使用 q4_0 方法）。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 convert.py models/7b/</span><br><span class=\"line\">./quantize ./models/7b/ggml-model-f16.gguf ./models/7b/ggml-model-q4_0.gguf q4_0</span><br></pre></td></tr></table></figure>\n<h3 id=\"运行交互模式\">运行交互模式<a title=\"#运行交互模式\" href=\"#运行交互模式\"></a></h3>\n<p>以下是运行模型的示例命令。启动一个交互式会话，可以在其中输入问题或语句，模型将生成回答或继续文本。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./examples/chat-7B.sh</span><br><span class=\"line\"><span class=\"comment\"># 或者使用自定义参数</span></span><br><span class=\"line\">./main -m ./models/7b/ggml-model-q4_0.gguf -n 256 --repeat_penalty 1.0 --color -i -r <span class=\"string\">&quot;User:&quot;</span> -f prompts/chat-with-bob.txt</span><br></pre></td></tr></table></figure>\n<p>请注意，这些步骤假设您已经具备了必要的 Python 依赖项，并且已经正确安装了 Python 环境。如果您遇到任何问题，请确保遵循文档中的详细说明。此外，由于 LLaMA 模型的权重较大，确保您的设备有足够的存储空间来保存这些文件。</p>\n<h3 id=\"错误处理\">错误处理<a title=\"#错误处理\" href=\"#错误处理\"></a></h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">user@mbp llama.cpp: python3 convert.py models/7B/</span><br><span class=\"line\">Exception: Vocab size mismatch (model has -1, but tokenizer.model has 32000)</span><br></pre></td></tr></table></figure>\n<ol>\n<li>\n<p><strong>修改 params.json 文件</strong>：<br>\n在 <code>llama-2-7b-chat</code> 文件夹中找到 <code>params.json</code> 文件，并将其中的 <code>&quot;vocab_size&quot;</code> 值从 <code>-1</code> 改为 <code>32000</code>。<a href=\"https://github.com/ggerganov/llama.cpp/issues/3900#:~:text=This%20is%20an%20easy%20fix,06%2C%20%22vocab_size%22%3A%2032000\" target=\"_blank\">更多信息</a></p>\n</li>\n<li>\n<p><strong>修改转换脚本</strong>：<br>\n如果在 <code>params.json</code> 文件中检测到 <code>vocab_size</code> 为 <code>-1</code>，可以在转换脚本中直接删除 <code>vocab_size</code> 的值，并使用 <code>tok_embeddings.weight</code> 作为回退。<a href=\"https://github.com/ggerganov/llama.cpp/issues/3900#:~:text=I%20also%20hit%20this%20and,weight\" target=\"_blank\">更多信息</a></p>\n</li>\n<li>\n<p><strong>应用修复补丁</strong>：<br>\n对于这个问题，有一个提交的修复补丁，即当检测到 <code>vocab_size</code> 为 <code>-1</code> 时，简单地从解析的 <code>params.json</code> 中删除它的值，并回退使用 <code>tok_embeddings.weight</code>。<a href=\"https://github.com/ggerganov/llama.cpp/issues/3900#:~:text=,3900%E3%80%91\" target=\"_blank\">更多信息</a></p>\n</li>\n</ol>\n<hr>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">user@mbp llama.cpp: ./quantize models/7B/ggml-model-f16.gguf models/7B/ggml-model-q4_0.gguf q4_0</span><br><span class=\"line\">llama_model_quantize: failed to quantize: unexpectedly reached end of file</span><br></pre></td></tr></table></figure>\n<ol>\n<li>\n<p><strong>确认转换文件完整性</strong>：<br>\n确保下载的 <code>.pth</code> 文件完整无误，并成功转换为 <code>ggml-model-f16.gguf</code>。<a href=\"https://github.com/ggerganov/llama.cpp/issues/2361#:~:text=,q4_0.bin\" target=\"_blank\">更多信息</a></p>\n</li>\n<li>\n<p><strong>运行量化命令</strong>：使用如下命令进行量化：<br>\n<code>./build/bin/release/quantize models/ggml-model-f16.bin models/ggml-model-q4_0.bin 2</code><br>\n确保路径和文件名正确无误。<a href=\"https://github.com/ggerganov/llama.cpp/issues/2361#:~:text=PS%20F%3A%5Cwork%5Cllama.cpp%3E%20.%2Fbuild%2Fbin%2Frelease%2Fquantize%20models%2Fggml,q4_0.bin%27%20as%20Q4_0\" target=\"_blank\">更多信息</a></p>\n</li>\n<li>\n<p><strong>检查代码更改</strong>：一个成功解决此类问题的案例中，问题出在 <code>unordered_map</code> 上。更改它为常规的 <code>map</code> 可以解决问题。在 <code>llama_vocab</code> 和 <code>llama_load_tensors_map</code> 的两处代码中进行了更改。<a href=\"https://github.com/ggerganov/llama.cpp/issues/2361#:~:text=Finally%20success,two%20places%2C%20llama_vocab%20and%20llama_load_tensors_map\" target=\"_blank\">更多信息</a></p>\n</li>\n</ol>\n<hr>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">user@mbp llama.cpp: ./examples/chat-7B.sh</span><br><span class=\"line\">gguf_init_from_file: invalid magic characters .</span><br><span class=\"line\">error loading model: llama_model_loader: failed to load model from models/7B/ggml-model-f16.gguf</span><br></pre></td></tr></table></figure>\n<p>根据您提供的错误日志，问题出现在加载模型的过程中。错误信息指出 <code>ggml-model-f16.gguf</code> 文件无法被正确加载。</p>\n<p>这可能是由以下原因造成的：</p>\n<ol>\n<li>\n<p><strong>文件损坏或不完整</strong>：确保 <code>ggml-model-f16.gguf</code> 文件没有在下载或转换过程中损坏。重新下载或转换该文件可能有助于解决问题。</p>\n</li>\n<li>\n<p><strong>文件路径错误</strong>：确认 <code>ggml-model-f16.gguf</code> 文件的路径正确。错误信息中提到的路径是 <code>models/7B/ggml-model-f16.gguf</code>。请确保文件确实存在于该路径，并且命令行工具的工作目录正确。</p>\n</li>\n<li>\n<p><strong>文件格式问题</strong>：如果文件格式不正确或与期望的格式不匹配，也可能导致这个错误。请确保您使用的是正确的转换脚本和参数来生成 <code>ggml-model-f16.gguf</code> 文件。</p>\n</li>\n</ol>\n","prev":{"title":"在 Linux 系统上实现一个简单的字符设备驱动程序","link":"2023/11/25/oc_linux_driver"},"next":{"title":"Hexo + GitHub Action 自动化博客部署","link":"2023/11/18/oc_hexo_inside_deploy"},"plink":"https://vincenteliang.com/2023/11/23/oc_run_llama2_local/","toc":[{"id":"下载模型文件","title":"下载模型文件","index":"1"},{"id":"安装-llama.cpp","title":"安装 llama.cpp","index":"2"},{"id":"准备数据","title":"准备数据","index":"3"},{"id":"转换模型格式","title":"转换模型格式","index":"4"},{"id":"运行交互模式","title":"运行交互模式","index":"5"},{"id":"错误处理","title":"错误处理","index":"6"}],"reward":true,"copyright":{"author":"Vincente Liang","link":"<a href=\"https://vincenteliang.com/2023/11/23/oc_run_llama2_local/\" title=\"Llama 2 本地部署指南\">https://vincenteliang.com/2023/11/23/oc_run_llama2_local/</a>","license":"Attribution-NonCommercial-NoDerivatives 4.0 International (<a href=\\\"https://creativecommons.org/licenses/by-nc-sa/4.0/\\\" rel=\\\"external nofollow\\\" target=\\\"_blank\\\">CC BY-NC-ND 4.0</a>)","published":"November 24, 2023","updated":"July 1, 2024"},"reading_time":"1231 words in 8 min"}